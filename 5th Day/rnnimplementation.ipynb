{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137629 chars, 81 unique \n"
     ]
    }
   ],
   "source": [
    "data=open('kafka.txt','r').read()\n",
    "chars=list(set(data))\n",
    "data_size,vocab_size=len(data),len(chars)\n",
    "print (\"data has \"+str(data_size)+\" chars, \"+str(vocab_size)+\" unique \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 0, 'v': 1, 'g': 2, 'w': 3, 'S': 4, '\"': 5, 's': 6, '§': 7, 'r': 8, '5': 9, '3': 10, '9': 11, 't': 12, 'f': 13, 'R': 14, '@': 15, ',': 16, '/': 17, 'm': 18, 'n': 19, '7': 20, '2': 21, 'a': 22, 'q': 23, ';': 24, 'L': 25, 'e': 26, 'U': 27, '4': 28, 'y': 29, 'Ã': 30, '6': 31, 'F': 32, 'u': 33, '.': 34, '\\n': 35, 'x': 36, \"'\": 37, 'j': 38, '?': 39, 'p': 40, 'l': 41, 'M': 42, 'J': 43, 'Y': 44, 'A': 45, '8': 46, 'X': 47, 'i': 48, '0': 49, '!': 50, 'h': 51, 'T': 52, 'N': 53, 'd': 54, 'V': 55, '*': 56, 'O': 57, 'P': 58, 'K': 59, 'G': 60, ':': 61, '1': 62, '-': 63, 'E': 64, 'k': 65, ')': 66, 'H': 67, 'c': 68, 'z': 69, ' ': 70, 'W': 71, 'I': 72, 'C': 73, '(': 74, '$': 75, 'b': 76, 'D': 77, '%': 78, 'B': 79, 'Q': 80}\n",
      "{0: 'o', 1: 'v', 2: 'g', 3: 'w', 4: 'S', 5: '\"', 6: 's', 7: '§', 8: 'r', 9: '5', 10: '3', 11: '9', 12: 't', 13: 'f', 14: 'R', 15: '@', 16: ',', 17: '/', 18: 'm', 19: 'n', 20: '7', 21: '2', 22: 'a', 23: 'q', 24: ';', 25: 'L', 26: 'e', 27: 'U', 28: '4', 29: 'y', 30: 'Ã', 31: '6', 32: 'F', 33: 'u', 34: '.', 35: '\\n', 36: 'x', 37: \"'\", 38: 'j', 39: '?', 40: 'p', 41: 'l', 42: 'M', 43: 'J', 44: 'Y', 45: 'A', 46: '8', 47: 'X', 48: 'i', 49: '0', 50: '!', 51: 'h', 52: 'T', 53: 'N', 54: 'd', 55: 'V', 56: '*', 57: 'O', 58: 'P', 59: 'K', 60: 'G', 61: ':', 62: '1', 63: '-', 64: 'E', 65: 'k', 66: ')', 67: 'H', 68: 'c', 69: 'z', 70: ' ', 71: 'W', 72: 'I', 73: 'C', 74: '(', 75: '$', 76: 'b', 77: 'D', 78: '%', 79: 'B', 80: 'Q'}\n"
     ]
    }
   ],
   "source": [
    "#Dictinary\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print (char_to_ix)\n",
    "print (ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print (vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperParameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modelParameters\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #hidden to next hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #hidden to output \n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " Ws%?*6@QEaB)p\"f%-vO:qaCVckjYBx zs( vw9Yhx2wi42v;fqDMvVtL(-KfE)dXp -yOL,v6Pc.m2eO6rtV97?*aD(YfdgPu@Eb7FHig6K6,ywDgBO)S.3IsI0/YiMVMyaWv(mKK*j%Cs6SmTgCTIY-MJUW;q(,r7CNEWBKbGezWnwUip§ed5YT;\"2h?DWNDlHB29Ma \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [57, 19, 26, 70, 18, 0, 8, 19, 48, 19, 2, 16, 70, 3, 51, 26, 19, 70, 60, 8, 26, 2, 0, 8, 70]\n",
      "targets [19, 26, 70, 18, 0, 8, 19, 48, 19, 2, 16, 70, 3, 51, 26, 19, 70, 60, 8, 26, 2, 0, 8, 70, 4]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print (\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print (\"targets\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.861241\n",
      "----\n",
      " GCqHC'p!yCH8@PnÃiK4ndp%CdnY!xzt'IPW/eV)@geFroj.Kul@(h*\"F5YoQmg3Te3CM9V17nEK3K,@)Uh'*x$ Mgwl0JB?vvHz§ogGsi3ek,%2wc%Px!P?-dbukF'?uQUPXnWE77 pBIc;Q;ewlbl@9jyphc;yQaa(f*t$(st/6V7k6y1y5@1plÃlxBDz\"o)Tc1TLhe \n",
      "----\n",
      "iter 1000, loss: 85.269788\n",
      "----\n",
      " is the,gouve pe wnog . Bo gr. dedcoevelitktod ae eo tow borr he, this tDed f ke fives sy uro for ve mire heov routoanl \"muctoe. wfonge kin yke aitd , omisk cry h reyir anl hovfive carinn core tre ln y \n",
      "----\n",
      "iter 2000, loss: 68.500019\n",
      "----\n",
      " heth't aile hiifr ted qur. shish. hed witheethe ,uemerherhe shtN the\"demhenchepederehe she hidild gisgelhenhastlegesyoutoy cthe y yile nn wiwhe has tesime love hid ere'tetheedememiiHo nelghirheched sn \n",
      "----\n",
      "iter 3000, loss: 60.061205\n",
      "----\n",
      " w ucuy fuskd, licome:ly a foro sudonn Pathe gousteto mond'ur hoes barded bumter; -hipin loswind ofe ons atr biste kusilo troorlatlenry, domlenery athed todly soum woult:eo,k; berl sutianlhale mim of d \n",
      "----\n",
      "iter 4000, loss: 55.889279\n",
      "----\n",
      " hel an hegomithinid wherrathoond thin mor't int withen ir ap he blaer wtooheraflveco hithen sirtetht stotd he hitr'tprrourithouadeem beet'l ary aathom, bise of abegatana cert. and nithexciwed thoor, h \n",
      "----\n",
      "iter 5000, loss: 58.175510\n",
      "----\n",
      "  wren itepaniter1. wouichc rovactre. 1.udrp\n",
      "sos. Gracsr\"1up arbEoc1. 1ropeag a1d anerptinge s1Eunt in Theyout1. 11 a. tout. 11 tow outh 1t 1s Phou1erope! phe par the perwhomay Phitmepior wif otiin dis \n",
      "----\n",
      "iter 6000, loss: 58.646104\n",
      "----\n",
      " on and be anh ro candy onc peeut lare eiou wond no dis bomess bean't out then he wamn ald'd Gole and ngo norhandaveds mos. in ans to roons he to \"Hif oug tow at onn he he faing gted ans sorp er be hat \n",
      "----\n",
      "iter 7000, loss: 54.379752\n",
      "----\n",
      " inging doont Gregor paplrave sivound qu his tht aed bising waike as bestat isrlougdo maram thagat waume ceinger nha looc cveatad he siasfevereg cory thing wor gofas ngat ne in ind soeh regt washtow Gr \n",
      "----\n",
      "iter 8000, loss: 51.404035\n",
      "----\n",
      " omage camlr's emen to anle and rery toe hus upent ste ind blapeise sils thas matgat beenbitt was beem eed yoone soped vo het he sirp\n",
      "sto weaked le whes is leved cerould to it almils; hed to forei the  \n",
      "----\n",
      "iter 9000, loss: 49.953120\n",
      "----\n",
      " duleht, wout sound oos andtrevee his woom to dotorered do thes outithe's room hascatheeton ntoestevrearetsestorreist lect of haochent; - hitherecor, her at oupthoughe Ohe titaiciale, her in that saren \n",
      "----\n",
      "iter 10000, loss: 49.318960\n",
      "----\n",
      " imt sped ItS rood cow bying to thint to at herem them ped boey kith heintoner sininsted Greguntimar oplen en;er thit ip srond -pedo the bate anmong iupredts. She forder yough thomo \"Sheje -te glet ser \n",
      "----\n",
      "iter 11000, loss: 55.465174\n",
      "----\n",
      " esyonst Daodpyie\n",
      "\n",
      "eht fort witainang\n",
      "his co mate of and dre aqt Gut outtye/ngct Grted.\n",
      "\n",
      "Grdg- o8ce bavat a dopt cencs wof usd, pulksldet of tiss.  1ighm\n",
      "- s. wvyot cane thsirs the: the tilr lecout lus \n",
      "----\n",
      "iter 12000, loss: 52.276964\n",
      "----\n",
      " stos it the thet uve! Ble il to had it Gregly prevem a wourd. He peeno co quised to the woonent infprot; wer as to to do hounst, this shisk lhes is how w ofe 3ivered plore cles nonp ye forat wis wo do \n",
      "----\n",
      "iter 13000, loss: 49.159037\n",
      "----\n",
      "  sustim outgor. \"Ontseadaing no hoy -baJcheng hauless to offalfwing, iif Gregor if tott muthe hach oneh wase Gregoryclfy thoughom aple pustalluzk pethay he coomed soonere- duy agas alr.\n",
      "\n",
      "Bhes that her \n",
      "----\n",
      "iter 14000, loss: 47.426890\n",
      "----\n",
      "  in hay has pace chisw would alw she any \"Hay. Hell and, soy firsy whint and his lack as nathar wimfing agh s surll he fute mape fad loth sufrie being to she would in thag aed wloald as nowen purras c \n",
      "----\n",
      "iter 15000, loss: 46.721436\n",
      "----\n",
      "  ave gatirms Gregor the, he some to the froe eftgo ext merat taintt and it hes he mart Gregor's hamped to he elf:, daches tfed to mork thou the dor, lits abr blet the slyint bede Gregor fard the pame  \n",
      "----\n",
      "iter 16000, loss: 49.505483\n",
      "----\n",
      " ofeSt-itithat thit the pistedBent theicen lofingss. Nors an\n",
      "\n",
      "ounen sis appeln.\n",
      "Saakentsid cavapenen that betedely, k oorlyted the whit Prey vriwate hus iniend Proje the Gote\n",
      "undtyouct\n",
      "Corico youn ive  \n",
      "----\n",
      "iter 17000, loss: 51.064554\n",
      "----\n",
      " is oxiepllly bus row of tr ok thauf gork's fook was his pusiesdy himleably\n",
      "wachor it a purery d\" Bett igot if hem quzdedlnt fong\n",
      "wable wis sampefwegbt cond the wor's lo had facktrce chot owh iA his go \n",
      "----\n",
      "iter 18000, loss: 48.380401\n",
      "----\n",
      "  \"Gxegor allplation laght Gregor brether ad raistor's was tod toer one the that her as to pevare- doon, and agourd rave to ouppeatir ssath fore of left jatheH. No withtm sargoughime with to garled. be \n",
      "----\n",
      "iter 19000, loss: 46.331648\n",
      "----\n",
      "  wad foaf erayny hed cpeed theadistr the hive the a rai imer whed and his gaster his that who hissily wow tre unswoubdterratht sard be to buted abledre ftaint titht ert bufforbed himplly to stry hude  \n",
      "----\n",
      "iter 20000, loss: 45.578440\n",
      "----\n",
      " t ever foan Gregor't wiok her his meshed bece thro lale outter and to the rishid that she math. aves this !irhaip his had sifarenthithe him lich as sithed une hed the not sither harker Gregor's sheich \n",
      "----\n",
      "iter 21000, loss: 45.361218\n",
      "----\n",
      " y firly aly to proomle. And tist nither of and. in on thas nomely but herd bealld wouther his s..\". and fore ave out jopen tive and aid shas lade rood troves; more cublaked ofter gole ba, rould. Samp  \n",
      "----\n",
      "iter 22000, loss: 50.088718\n",
      "----\n",
      " nbom outt aboneghe-caily-ter ty ge\n",
      "\n",
      "Hish falf Uname reting-t\" Prouss\n",
      "fonion any do\n",
      "voyt, Project IT seabaten to scasloon of.  recoute\n",
      "\n",
      "Greg-t De fake a priciel to os or to the or Projoat - coreer of o \n",
      "----\n",
      "iter 23000, loss: 48.181125\n",
      "----\n",
      " int avere sirrest that of aflither lith plerst -t notioud Mred him to woating. The mowe becfavetither dott was sarel?er hour, recperas beitlateor w\"y bed aals to pust!\", illingsly on thed to a rverly  \n",
      "----\n",
      "iter 24000, loss: 45.814181\n",
      "----\n",
      " rmich lesren tadly her dist ander wath he hacpombaie wauf repltially Galssely ree but cley could upely werine amse exteess evey wisken evence what he her loiesyt as tot you tuainenesming radiouiw Whe  \n",
      "----\n",
      "iter 25000, loss: 44.374468\n",
      "----\n",
      " hing un to mickse't to hild that puthing thed a compopy Gregor't a dand fappey way owat the preiche out, and bemonitrait becould wores an hat this wouly for abree lowby got but the raadaly voy and whi \n",
      "----\n",
      "iter 26000, loss: 44.084999\n",
      "----\n",
      " ng, sistifo andort; he was ever, so could the at opsing? ernent even ewforss twewly gons us seamef it fale weet no ho room to he maid ham now sicamady farred. Him, Gregor sinned leamen did way fanger  \n",
      "----\n",
      "iter 27000, loss: 46.023015\n",
      "----\n",
      " ouecertions, mated sheidecatering wenting  as. \n",
      "Guteovidiigher soveredening un, becerid, wout jeitrenil\n",
      "\n",
      "Olr anc: Pronn themeathert ladiout it they thim acact\n",
      "Lsis wacus ay inboc, to no his loog or en \n",
      "----\n",
      "iter 28000, loss: 47.729672\n",
      "----\n",
      "  They'ng indmaghtAbeca It\"op tho hay vouklectird br alquichint it foumiin doomed to prog andor. But inbop- to hand wasser tosade eller not hathanitht ael hims of sad father, his bound \"rane hne so no  \n",
      "----\n",
      "iter 29000, loss: 45.701547\n",
      "----\n",
      "  of and cousd wither. He\n",
      "ous instseatly ouvire at they out and and feat couldcragheed anbed the could now war, wa door, witd that tyundly doncued abreate unet the statry that uld of onthands fave hap  \n",
      "----\n",
      "iter 30000, loss: 43.945988\n",
      "----\n",
      " srongitive so now of whew as raok us but repelly riy sent plealy though Gregor's hempint of the adr, a dirm. He had that he'med you oft wereed his fared to propitid, thel's lidly. He his feahte pray,  \n",
      "----\n",
      "iter 31000, loss: 43.457661\n",
      "----\n",
      " emated munimer. Some were werint hangeres then siss eleplo dist wast. Nour aly he ham a dealeded tty vele to beife and has was been andn and in himpelen and had sure thet they nomed. If it lyichomen j \n",
      "----\n",
      "iter 32000, loss: 43.345670\n",
      "----\n",
      " ne was beition the doom poplod then a donten had in er, simene ede with, Gregor he cained no Gregomen seint to rothers the had it dian spanat\", then seare and: u ford bely or the was spent by strinr o \n",
      "----\n",
      "iter 33000, loss: 47.139957\n",
      "----\n",
      " tsel on trecey\n",
      "olle lequsutiGutenbengtre., SorellectAWs.  IBerede chater or that or repllack aE, a cablug-\n",
      "*IT-D/. DISCstaber dedaristens thad h0t wereer vateat andadiat Sathont Mor\n",
      "on quicts alf conl \n",
      "----\n",
      "iter 34000, loss: 46.015790\n",
      "----\n",
      " l go cleined of that biter of they, his faremmorm eredre have timaiting he gall \"saukine to checkng-d the? And wimem aly to ho has his andya to cout thed madle whais plofr he wark aff and fare? whaten \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 43.925263\n",
      "----\n",
      " y sarday the sso-t therely be he had and wien ke sle they dillshy ther's lede tteing vered thes to he whians on way fave rusted oids wamt's meed, of and efly moll to satsunt to moorse soant the mnow t \n",
      "----\n",
      "iter 36000, loss: 42.625828\n",
      "----\n",
      "  riselfid cose helrssed was him yourserfade do fime agk as his hak as badse has tivered him to to ollsious, and white; balling, in his dould veadnedm, waschof the wath dider Gut was ain would sithings \n",
      "----\n",
      "iter 37000, loss: 42.571276\n",
      "----\n",
      "  His seach humony nowher it sister.e\" some. Hey ontor a; shom'en comsalunicry.\n",
      "\n",
      "Nher mouper hamprshing upakly, as stoln's his theur stery, and surenter at back him imselt lime the rither ever now, eve \n",
      "----\n",
      "iter 38000, loss: 44.140091\n",
      "----\n",
      " fte dod cley on theissrard; or opyGy things and the fricarer the firt, parmatelect this stey,\n",
      "\n",
      "Gregored and\n",
      "Guteps waytevered hid paveeded.\n",
      "\n",
      "Bee fillsither the casssingain ded of ous poplaged desaik G \n",
      "----\n",
      "iter 39000, loss: 45.853747\n",
      "----\n",
      " dans of this been way he was and his keablo's his oot and endin onto lpaceaking treneassivereall of his shous stiat he chot had comming he hod could, dess, kenbl \"Shing the thirk, abrory i got onditio \n",
      "----\n",
      "iter 40000, loss: 44.132984\n",
      "----\n",
      " y's lhoupd ttoup and anw hrof the and carde ste!\", beth, and comly it's very the stible farss bce allowed to hit's he dixtall the; shis jated kroak, able their was it Grugor wis, thound preate in was  \n",
      "----\n",
      "iter 41000, loss: 42.533713\n",
      "----\n",
      "  loing fore gemplans to the ring not best shind he was becthe than toous aletpehing him his flane, im her sime uncome hid the to the clare clifuskly, ank so combeaties, to effor pofy shough whouber, r \n",
      "----\n",
      "iter 42000, loss: 42.138885\n",
      "----\n",
      " t makn to \"she couch dube soom, whought their room bedereesire sister wo thot mome had beening the corninnews wall, buchad whot his wiukeh. Are and moecurned they's unter h aftent he warrong har weree \n",
      "----\n",
      "iter 43000, loss: 42.111635\n",
      "----\n",
      " otion?\", his father; if his cloted thim, was bufto beh it useded, and jeste and ally, and for his siening one train Mope for ovening and was, on it\n",
      "or fraily, out it selfovie deew his comppeentanlle w \n",
      "----\n",
      "iter 44000, loss: 45.335685\n",
      "----\n",
      " e nef axmorcation.  3\n",
      "sencedly ousapy not infove in or expredingstions on ermaying and abde off\n",
      "roncele is\n",
      "ormallrenaw Suchack dis videatt, muscedv\n",
      "Noned of did\n",
      ". \n",
      "Ceniat cars of the unberneds mading. \n",
      "----\n",
      "iter 45000, loss: 44.568685\n",
      "----\n",
      " un the could \"Whinued or? That his party from he surpingor ould prothem, creably his situpe the car his, maked rook to bething as dist could pand had a dor yout cleacaineless of therc ascife want dott \n",
      "----\n",
      "iter 46000, loss: 42.661747\n",
      "----\n",
      " ewinded, was would soswow as exced the enckevenee ver'les fend allaos there thimseen thees, ore lit the byet'e say would mested bectire ly sisfivect ayy did as he risn tualuramed and as leadiled and s \n",
      "----\n",
      "iter 47000, loss: 41.428839\n",
      "----\n",
      " eat hel. He rat had wallsay liny net bryout that he hid been expranit; hid poshen hais his to he hand.\" abd there salfootly of is Gregor to marsers he would ain. The and beent from - \"Sarme trowe and  \n",
      "----\n",
      "iter 48000, loss: 41.417993\n",
      "----\n",
      " ifareem thim; Undiniche suris so that, he caint it. He hince tyoug; beactly his inst, unter tokre they semareaed. His whon fored ofton could noor, dowavion, they him fus lot eroug no who had newa his  \n",
      "----\n",
      "iter 49000, loss: 42.779692\n",
      "----\n",
      " er's fortile and was noro-tleds this trearly namenif for sroject Chinge a any\n",
      "oor cuncedtated works laculy wasiss of washe anf deem The proundr and work coments.  lacking.w\n",
      "Intede Grigh wa friely not  \n",
      "----\n",
      "iter 50000, loss: 44.543755\n",
      "----\n",
      " lose you do be nenew and regatint wedt lext dedane seving, of thorress he main of the was shoughn ain rick.  IBea sucs or would atilequ, clerk: The beebir the kious stryere un agained tuslut then his  \n",
      "----\n",
      "iter 51000, loss: 43.032418\n",
      "----\n",
      " y ogat lofubining wa drong. The fampcaing.\n",
      "ABatI for in hald, being coule no the effort waf dive, and in was with out recearly cemshids toum, plopl wattly she Graip batile tull samsl to to witht of si \n",
      "----\n",
      "iter 52000, loss: 41.514217\n",
      "----\n",
      " eld forgouting ite room a roomed dome woulder on porefuvermind on tist, scarlaking refthitiound thenitgout up into to being to as she cittled as not that hamjel as sourdented oflither repid, pame by.a \n",
      "----\n",
      "iter 53000, loss: 41.220351\n",
      "----\n",
      " oning undsed mare, the could plookef trepseese. \"On they mowered of sgre. The harrewnle whither up stiris whictletshaute soutser his to's room and thems, evenet ustioning her slach of a ked at expin w \n",
      "----\n",
      "iter 54000, loss: 41.174994\n",
      "----\n",
      " o ste, some hardly dime. \"Roon it\", his income ulorearts the sequeney. For ofath, a confing imone the backrthind noth morn of thly k what thened backrsy, and from to clipted lof her the semiade out.\"\n",
      " \n",
      "----\n",
      "iter 55000, loss: 43.927116\n",
      "----\n",
      " \n",
      "iln risnitgotingrthend macrte\n",
      "fore\n",
      "projecin: Gotectire lowations it rowabotreping-tmon!\n",
      "N) with\n",
      "D as allanilyeng agreastaped an asconaemais work so the couy roverattall dhousorawitavist the sceah the \n",
      "----\n",
      "iter 56000, loss: 43.535983\n",
      "----\n",
      "  in ite oft no cettered in the pmured himplice. Iurdor buckiched, listinh Buts of the could shemous of the pappect.\n",
      "\n",
      "1If had farlsple and shat humjut the thed,. If this eremellat was a disted, nothend \n",
      "----\n",
      "iter 57000, loss: 41.744691\n",
      "----\n",
      "  theless where onfore with was and there speed ain, the pusiunded iny come ass - concer. But fom the lownun before with on the stipe would sus could sepeaning, had only mar he just and betant dist the \n",
      "----\n",
      "iter 58000, loss: 40.600147\n",
      "----\n",
      " ft Gregor preally had his t; Gutgre, and mableach enat he, \"Whime hid cleaationssooking lyad thein fren ovever the came agher bar.\n",
      "\n",
      "Gregor had bets we!a his hell sours bofor that a fint the allly with \n",
      "----\n",
      "iter 59000, loss: 40.583094\n",
      "----\n",
      "  ofterated pardess was him spermathor, and wabk his to that from over.\n",
      "Mrengs Gregowly fay he was toor hele op prrous so for and cam adouch. in cte boenen his permade so conly wele here hay not. The q \n",
      "----\n",
      "iter 60000, loss: 41.794053\n",
      "----\n",
      " elmeven wingioninge the ofather so uppinist\n",
      "coundr eve yop bein whiled becomed posseated the snotion. - moming usppemed and wass iny\n",
      "co fepent yow coverick that grice slong the Uniting.  Undover you o \n",
      "----\n",
      "iter 61000, loss: 43.539544\n",
      "----\n",
      " rreen any under of this. That thation, mathere no hear to him,. Becturr you ngred he refteved could undlaiten tlictle, would the the onjer's ccering in wall clargonted conthed, stime and nos, at this  \n",
      "----\n",
      "iter 62000, loss: 42.207801\n",
      "----\n",
      " shads was a in work, to no gec, bight to his mimod unane, quivenbe gow here has wauther, stivelly, that noting stick, whoter to cledes\", and that he was have to his hark orhtew himpltianings as was ab \n",
      "----\n",
      "iter 63000, loss: 40.746328\n",
      "----\n",
      " n out afforttupie. veres. Gregor's by Gregor whetent, is himseld wow the ith, thes ighte ot to the mores that not no what and he starmou, the wome te gitt his syadk ble all dippewssond and from the th \n",
      "----\n",
      "iter 64000, loss: 40.506842\n",
      "----\n",
      " y somesiany broous to be flee flod herr wast asplapeer, chanke as in. When now the could him whele, has she had and foor the epponey, him roverused arked he hay onter. He had fore to pliester. And hot \n",
      "----\n",
      "iter 65000, loss: 40.448017\n",
      "----\n",
      " same frectstookinger, and ramstate, and murhay the, dowly was of and shout distrenre provescasther, rais coume his rempainspmightmord headedsemw evertey doodithend frack fervive a conly work brelmore. \n",
      "----\n",
      "iter 66000, loss: 42.874708\n",
      "----\n",
      " f condec clapibl the ro nex on at\n",
      "oulto fext him th of work the wioving tim wos thit (mection wark the.  Theren the Formatiousist to the fork turts immeally ts agren acciumor and of eay ovioulatat, au \n",
      "----\n",
      "iter 67000, loss: 42.721004\n",
      "----\n",
      " now\", more \"somast was gon!\"\n",
      "wime can his atat, hamss; ang contons if handsims in, or accepblesuld. Thely reacialend; about wan with ackessy, whaty cauviof dow\" his miras jurday though the ger gensbyo \n",
      "----\n",
      "iter 68000, loss: 41.035608\n",
      "----\n",
      " e in thint he cladeacter. Suncould.Buring but Gregor could oney, stally, as be best, some a distfeed ancerane and his hectall desely if even, puesl disty, s searions on in the posicesson offectreeve a \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 39.954328\n",
      "----\n",
      " onning, at sham any anget heame appested just of not at won the was soads up of his mother oucanly he was head him to st to the Souslised stotily so her at ut was, in hay Gragh tle of hat he conyy fem \n",
      "----\n",
      "iter 70000, loss: 39.906030\n",
      "----\n",
      "  whought jost in shor fiacesway could and with fan ores hole to shed to some forted but the something to kissid to wiomed obreas the blackents him olvout, Gregorge and bockripn sis torest her fall wat \n",
      "----\n",
      "iter 71000, loss: 41.081453\n",
      "----\n",
      " rsy his for sie breining enes upery. Masiet works no it by more ewad wrowerare.  IR PLOut wher had will that no they beyout f by the proilver, ily spepun of ilsoo, the iftece the roon proruping emyepf \n",
      "----\n",
      "iter 72000, loss: 42.735919\n",
      "----\n",
      " inly then had fene hamft the fiatelly, he he, net in the off.E serfadr. I comy but Sal tadded to elfwise he strint no that eleare - heain his negher the could bee claip. He bet- Gregor ho\n",
      "chatently on \n",
      "----\n",
      "iter 73000, loss: 41.517980\n",
      "----\n",
      " t have to aceathen efforgth suce in he ceattioght lithed, all capen baid vining hel they he waghtwain that thertern of thrred it Gregor staning uf his sister othat'n the this into the him tusl, and ha \n",
      "----\n",
      "iter 74000, loss: 40.113877\n",
      "----\n",
      " ing, the em even on the fork that he was; cony whou. Gregor hear nolusion? Bother the lonkt to the kive ap yow oves Gregor's nither but the dedaly on in othioved to had was youns imfeme the batly at c \n",
      "----\n",
      "iter 75000, loss: 39.974188\n",
      "----\n",
      " th ted groks be they had mapuper thes f staght h\n",
      "pearing thengt work from his fare and reave it beessive headry some and was lyew\", cont\"oll farr hoer he con for deover the pen war the waited to paze  \n",
      "----\n",
      "iter 76000, loss: 39.845157\n",
      "----\n",
      "  risseeven any, as her bicaiced net and be to charded as he hor of \"Whe nor helr in room.n trear, shat in her therenwim ssie unpeed sure up int that in goundly. He ite thim, on hor firss, anped his sh \n",
      "----\n",
      "iter 77000, loss: 41.978072\n",
      "----\n",
      " flar cably\n",
      "wift; moded\n",
      "\n",
      "\n",
      "1Project GutenbeB. sprominations sumfime alceminct to frips thin nex a CCaffuls cony the arce. Conlr a ruffoncurionds unfort to meemed thaten tor\n",
      "the wantien.\n",
      "\n",
      "1.2.7 trajulten \n",
      "----\n",
      "iter 78000, loss: 42.072229\n",
      "----\n",
      " , be way worr how beds, while\"'s tee the doated livtere\", rey.  froush, in ould larand into could inpestioly's she the bespery shom! Fracky he who teswor dece, shan of herrally\n",
      "would froms but oser\n",
      "ay \n",
      "----\n",
      "iter 79000, loss: 40.433322\n",
      "----\n",
      " stilped agraytan gow loick was not couck tooutelese some sither then doom therech hard to a crepelless, at and evenist and so the would onlyend lose bee him. Ley he that musned of or oppeming he had s \n",
      "----\n",
      "iter 80000, loss: 39.411221\n",
      "----\n",
      " fightanted welled as and fither, and by the mather a lottonat of ilont his one grwath, edrey his buctrusl and bint for the cirtane, Would a pofy wing putter, a fidst: Gregor see to urmor. Ha be to ite \n",
      "----\n",
      "iter 81000, loss: 39.379533\n",
      "----\n",
      " and cequchatry trank that anjy tull been, somed. Gregor famper, they's roos and her anothart at the fullouthely he sime the but Gregor as he Hty sheitd rake of hat was a orhanim to the toly seen mome  \n",
      "----\n",
      "iter 82000, loss: 40.386921\n",
      "----\n",
      " dop it becomsions and rewarts owitlims of ack\n",
      "in me, - homeed only to yree priout of geveret works marhed no hid shoumorm om at of the full his rective firent bece. so emet the fert hard\n",
      "\n",
      "esce - Weal\n",
      " \n",
      "----\n",
      "iter 83000, loss: 42.080276\n",
      "----\n",
      " forcain of it on only whatlpos room agcoms thing there thing. Gregor  s breanings thiny to all as ofe fromsptate she fas was a wawnithast might opirgoun ot was ease with in the sersaciar, got with obl \n",
      "----\n",
      "iter 84000, loss: 40.978918\n",
      "----\n",
      " lt.  Grogn?\"Guten in a laws aftering that where if the witdle noors for a canc, sund ack banly had the was a kningh\" that and bee seen called. Abphing him gnsone may for ched him alreads, and he could \n",
      "----\n",
      "iter 85000, loss: 39.575461\n",
      "----\n",
      "  statgokion vodst from'aid floutht nondn his be wange, he had firked to been an she had food, in tho and seed the what he fams wink, stiemed she disted bockiveriver, turing that grating then she steme \n",
      "----\n",
      "iter 86000, loss: 39.539011\n",
      "----\n",
      " ntenbee withing now whode him himeesty kearctread lone besprenght was tlicking - when that he dapner was stive wan - theren ve to showed lemper approfrefstall home of wrotieke back not mowher ady were \n",
      "----\n",
      "iter 87000, loss: 39.325194\n",
      "----\n",
      " s of likenshingow ress ring nosted wn they that she dispis: Witdny. \"Wisly her beebmeake laces; oning dest al- conded dawalled it every theck not at where her but had under. He.  his a fabl he could h \n",
      "----\n",
      "iter 88000, loss: 41.217222\n",
      "----\n",
      " tender lent withat on the forme brtaten wild by hemmeaben ben and assta@e dofivings at axmated narearty slevenbwring\n",
      "to form work orquibnetibyaddation eveniegoutiorscintton.\n",
      "\n",
      "\n",
      "**0.  Lswath So arching  \n",
      "----\n",
      "iter 89000, loss: 41.517114\n",
      "----\n",
      "  placruped osten whither. The routs have he and way th, fort zestsoss ittly every an, Gregor, as so parly Gregor's regted there this but that the wosl he no worr's othing bet, he cair ofe it atainy\".  \n",
      "----\n",
      "iter 90000, loss: 39.911242\n",
      "----\n",
      " edy that \"Thels ture bapped yound the dads, you timy do?t hey vota we ettenaut ackiblv thing ontthay, were to more was even beinad the puent of her to dooche, got wither, himso hard the kead not laste \n",
      "----\n",
      "iter 91000, loss: 39.038304\n",
      "----\n",
      " xpro inthid had hund ouher as h muncounde that slowind whaten as a has saunded at of his brither neverssieted and came kefwer. Sichentone could raft tway she fort is wret celse inthap. He fay parfway  \n",
      "----\n",
      "iter 92000, loss: 38.933814\n",
      "----\n",
      " ven. He dle more. Andirg. Samsaus of bis dis compling and he we women and exinty, at well fake with to the was careatim the viot, of whal and the maid. Sho gounde. Bpeemings and the formoue to the dis \n",
      "----\n",
      "iter 93000, loss: 40.374529\n",
      "----\n",
      "  acous as with in inteves and recime in trint gonny as a cutsioughty with igrteds fromalite despirk on throng giten the incous nece westion.\n",
      "\n",
      "If, Gregor's amoor percemplear witdormed Projectibed aro a \n",
      "----\n",
      "iter 94000, loss: 41.820796\n",
      "----\n",
      " thing bake in that han all dist bequiet as he could Gregor the doled reosinim and stoxing pass!\n",
      " Fole gotly dolling and she froms looked againing the grite fire the pausly ovey other shald, but and eB \n",
      "----\n",
      "iter 95000, loss: 40.616829\n",
      "----\n",
      " ve\n",
      "with that his she mad of clealed mire as lyos, - reiniads was no the past tot te which, his ver to would ricunary; whech cotred, see have the\n",
      "rould from the ochers with by the flay a win.\n",
      "Were't ti \n",
      "----\n",
      "iter 96000, loss: 39.177501\n",
      "----\n",
      "  alalr, as did his priver his fill from no bres had the his sooidpreselsile the diably from not it. His mother their done, though. He see his that Gregorly was eveblem would it afferdact white as, he  \n",
      "----\n",
      "iter 97000, loss: 39.201943\n",
      "----\n",
      " ome bried, preas; him tweysiatered nowed time the cemapen down agaying miken some of themarseameff could not intenly thencargediserentime say buctirgat be butent, smoks a seminung bfercosed tooly a ha \n",
      "----\n",
      "iter 98000, loss: 38.964710\n",
      "----\n",
      "  and haxco any and hume but with, gride sisped then her allabley, at she mad bying their his mood cassed to survictly goving yoursion. Gregor his came. I the didls: whiy neut cousins, then leftly allo \n",
      "----\n",
      "iter 99000, loss: 40.717038\n",
      "----\n",
      "  white 9KM(ted the work nourcamperaneadning-trales.\n",
      "\n",
      "\n",
      "fround thad seef the wiff\n",
      "remeared.\n",
      "\n",
      "Becaice, all drapp, cousuret for exposs now reares out frovening noned.\n",
      "\n",
      "1.Ist the anfore the arx of zefort o \n",
      "----\n",
      "iter 100000, loss: 41.044148\n",
      "----\n",
      " ng and'cseps, wo\n",
      "chang the llokey the his id.\n",
      "\n",
      "\"If he have niy to sact wraidly to eneest of virk con that have hell the logey\", spent now? \"My.H.  fall have timan case his not thorned imber and agrer  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print ( 'iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
